{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "def get_embedding_module(imageSize):\n",
    "    # construct the input layer and pass the inputs through a\n",
    "    # pre-processing layer\n",
    "    inputs = keras.Input(imageSize + (3,))\n",
    "    x = resnet.preprocess_input(inputs)\n",
    "    \n",
    "    # fetch the pre-trained resnet 50 model and freeze the weights\n",
    "    baseCnn = resnet.ResNet50(weights=\"imagenet\", include_top=False)\n",
    "    baseCnn.trainable=False\n",
    "    \n",
    "    # pass the pre-processed inputs through the base cnn and get the\n",
    "    # extracted features from the inputs\n",
    "    extractedFeatures = baseCnn(x)\n",
    "    # pass the extracted features through a number of trainable layers\n",
    "    x = layers.GlobalAveragePooling2D()(extractedFeatures)\n",
    "    x = layers.Dense(units=1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(units=128)(x)\n",
    "    # build the embedding model and return it\n",
    "    embedding = keras.Model(inputs, outputs, name=\"embedding\")\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_network(imageSize, embeddingModel):\n",
    "    # build the anchor, positive and negative input layer\n",
    "    anchorInput = keras.Input(name=\"anchor\", shape=imageSize + (3,))\n",
    "    positiveInput = keras.Input(name=\"positive\", shape=imageSize + (3,))\n",
    "    negativeInput = keras.Input(name=\"negative\", shape=imageSize + (3,))\n",
    "    # embed the anchor, positive and negative images\n",
    "    anchorEmbedding = embeddingModel(anchorInput)\n",
    "    positiveEmbedding = embeddingModel(positiveInput)\n",
    "    negativeEmbedding = embeddingModel(negativeInput)\n",
    "    # build the siamese network and return it\n",
    "    siamese_network = keras.Model(\n",
    "        inputs=[anchorInput, positiveInput, negativeInput],\n",
    "        outputs=[anchorEmbedding, positiveEmbedding, negativeEmbedding]\n",
    "    )\n",
    "    return siamese_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(keras.Model):\n",
    "    def __init__(self, siameseNetwork, margin, lossTracker):\n",
    "        super().__init__()\n",
    "        self.siameseNetwork = siameseNetwork\n",
    "        self.margin = margin\n",
    "        self.lossTracker = lossTracker\n",
    "    def _compute_distance(self, inputs):\n",
    "        (anchor, positive, negative) = inputs\n",
    "        # embed the images using the siamese network\n",
    "        embeddings = self.siameseNetwork((anchor, positive, negative))\n",
    "        anchorEmbedding = embeddings[0]\n",
    "        positiveEmbedding = embeddings[1]\n",
    "        negativeEmbedding = embeddings[2]\n",
    "        # calculate the anchor to positive and negative distance\n",
    "        apDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - positiveEmbedding), axis=-1\n",
    "        )\n",
    "        anDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - negativeEmbedding), axis=-1\n",
    "        )\n",
    "        \n",
    "        # return the distances\n",
    "        return (apDistance, anDistance)\n",
    "    def _compute_loss(self, apDistance, anDistance):\n",
    "        loss = apDistance - anDistance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "    def call(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "        return (apDistance, anDistance)\n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute the distance between the anchor and positive,\n",
    "            # negative images\n",
    "            (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "            # calculate the loss of the siamese network\n",
    "            loss = self._compute_loss(apDistance, anDistance)\n",
    "        # compute the gradients and optimize the model\n",
    "        gradients = tape.gradient(\n",
    "            loss,\n",
    "            self.siameseNetwork.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siameseNetwork.trainable_variables)\n",
    "        )\n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "    def test_step(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        (apDistance, anDistance) = self._compute_distance(inputs)\n",
    "        # calculate the loss of the siamese network\n",
    "        loss = self._compute_loss(apDistance, anDistance)\n",
    "        \n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.lossTracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed Try1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " anchor (InputLayer)         [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " positive (InputLayer)       [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " negative (InputLayer)       [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " embedding (Functional)      (None, 128)                  2638105   ['anchor[0][0]',              \n",
      "                                                          6          'positive[0][0]',            \n",
      "                                                                     'negative[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26381056 (100.64 MB)\n",
      "Trainable params: 2790272 (10.64 MB)\n",
      "Non-trainable params: 23590784 (89.99 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# before training the model\n",
    "\n",
    "# initialize the image size and the batch size\n",
    "imageSize = (224, 224)\n",
    "\n",
    "# initialize the margin and loss tracker\n",
    "margin = 0.6\n",
    "lossTracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "embedding_model = get_embedding_module(imageSize)\n",
    "siamese_network = get_siamese_network(imageSize, embedding_model)\n",
    "siamese_network.build((None, *imageSize, 3))\n",
    "siamese_model = SiameseModel(siamese_network, margin, lossTracker)\n",
    "\n",
    "# compile the model\n",
    "siamese_model.compile(optimizer=keras.optimizers.Adam(1e-4))\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "def triplet_generator(base_dir=\"data\", image_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    A generator that yields triplets: anchor, positive, and negative images.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_dir: The base directory where the images are stored.\n",
    "    - image_size: The target size to which the images will be resized.\n",
    "    \n",
    "    Yields:\n",
    "    - A tuple of numpy arrays: (anchor, positive, negative).\n",
    "    \"\"\"\n",
    "    # Paths to the class directories\n",
    "    ben_dir = os.path.join(base_dir, \"ben\")\n",
    "    henry_dir = os.path.join(base_dir, \"henry\")\n",
    "    \n",
    "    # List of image filenames for each class\n",
    "    ben_images = [os.path.join(ben_dir, img) for img in os.listdir(ben_dir)]\n",
    "    henry_images = [os.path.join(henry_dir, img) for img in os.listdir(henry_dir)]\n",
    "    \n",
    "    while True:\n",
    "        # Randomly choose an anchor and a positive image from the same class\n",
    "        anchor_positive_class = np.random.choice([\"ben\", \"henry\"])\n",
    "        if anchor_positive_class == \"ben\":\n",
    "            anchor_path = np.random.choice(ben_images)\n",
    "            positive_path = np.random.choice([img for img in ben_images if img != anchor_path])\n",
    "            negative_path = np.random.choice(henry_images)\n",
    "        else:\n",
    "            anchor_path = np.random.choice(henry_images)\n",
    "            positive_path = np.random.choice([img for img in henry_images if img != anchor_path])\n",
    "            negative_path = np.random.choice(ben_images)\n",
    "        \n",
    "        # Load and preprocess the images\n",
    "        anchor_img = img_to_array(load_img(anchor_path, target_size=image_size))\n",
    "        positive_img = img_to_array(load_img(positive_path, target_size=image_size))\n",
    "        negative_img = img_to_array(load_img(negative_path, target_size=image_size))\n",
    "        \n",
    "        # Normalize the images\n",
    "        anchor_img /= 255.0\n",
    "        positive_img /= 255.0\n",
    "        negative_img /= 255.0\n",
    "        \n",
    "        yield ((anchor_img, positive_img, negative_img), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Devasy\\AppData\\Local\\Temp\\ipykernel_19076\\667390599.py\", line 35, in train_step\n        (anchor, positive, negative), _ = data\n\n    ValueError: not enough values to unpack (expected 2, got 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_images \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batchSize)  \u001b[38;5;66;03m# This is a simplified calculation\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Note: You might need to adjust steps_per_epoch based on your actual data and how your generator works\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainGen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexgsjzasl.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 35\u001b[0m, in \u001b[0;36mSiameseModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Unpack the data. Expecting data in the format: ([anchor, positive, negative], label)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     (anchor, positive, negative), _ \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Proceed with the existing logic for computing distance and loss\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Devasy\\AppData\\Local\\Temp\\ipykernel_19076\\667390599.py\", line 35, in train_step\n        (anchor, positive, negative), _ = data\n\n    ValueError: not enough values to unpack (expected 2, got 1)\n"
     ]
    }
   ],
   "source": [
    "# now code to train the model\n",
    "import numpy as np\n",
    "\n",
    "# initialize the image size and the batch size\n",
    "imageSize = (224, 224)\n",
    "batchSize = 32\n",
    "\n",
    "TRAIN_DIR = \"data\"\n",
    "# initialize the training data generator\n",
    "trainGen = triplet_generator(TRAIN_DIR, image_size=imageSize)\n",
    "\n",
    "\n",
    "# Calculate steps per epoch assuming you know the total number of images\n",
    "# For simplicity, let's assume each class in TRAIN_DIR has 3 images, making 6 images total.\n",
    "# Adjust this calculation based on your actual dataset size.\n",
    "num_classes = 2  # Adjust based on your dataset\n",
    "images_per_class = 3  # Adjust based on your dataset\n",
    "total_images = num_classes * images_per_class\n",
    "steps_per_epoch = max(1, total_images // batchSize)  # This is a simplified calculation\n",
    "\n",
    "# Train the model\n",
    "# Note: You might need to adjust steps_per_epoch based on your actual data and how your generator works\n",
    "history = siamese_model.fit(trainGen, epochs=10, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images in 2 classes\n",
      "Training on 4 images\n",
      "Validating on 2 images\n",
      "Initialized TripletDataGenerator with 4 images\n",
      "Initialized TripletDataGenerator with 2 images\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Devasy\\AppData\\Local\\Temp\\ipykernel_19076\\4041111102.py\", line 37, in train_step\n        (apDistance, anDistance) = self._compute_distance(inputs)\n    File \"C:\\Users\\Devasy\\AppData\\Local\\Temp\\ipykernel_19076\\4041111102.py\", line 8, in _compute_distance\n        (anchor, positive, negative) = inputs\n\n    ValueError: not enough values to unpack (expected 3, got 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 119\u001b[0m\n\u001b[0;32m    116\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam())\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[0;32m    123\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexgsjzasl.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 37\u001b[0m, in \u001b[0;36mSiameseModel.train_step\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# compute the distance between the anchor and positive,\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# negative images\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m         (apDistance, anDistance) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;66;03m# calculate the loss of the siamese network\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(apDistance, anDistance)\n",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m, in \u001b[0;36mSiameseModel._compute_distance\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_distance\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m----> 8\u001b[0m     (anchor, positive, negative) \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# embed the images using the siamese network\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msiameseNetwork((anchor, positive, negative))\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Devasy\\AppData\\Local\\Temp\\ipykernel_19076\\4041111102.py\", line 37, in train_step\n        (apDistance, anDistance) = self._compute_distance(inputs)\n    File \"C:\\Users\\Devasy\\AppData\\Local\\Temp\\ipykernel_19076\\4041111102.py\", line 8, in _compute_distance\n        (anchor, positive, negative) = inputs\n\n    ValueError: not enough values to unpack (expected 3, got 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "class TripletDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, image_size, num_classes):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.encoded_labels = self.label_encoder.fit_transform(labels)\n",
    "        self.image_data_generator = ImageDataGenerator(preprocessing_function=resnet.preprocess_input)\n",
    "        self.on_epoch_end()\n",
    "        print(f\"Initialized TripletDataGenerator with {len(self.image_paths)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.image_paths) // self.batch_size)  # Ensure at least one batch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_image_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.encoded_labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self._generate_triplet_batch(batch_image_paths, batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle the data at the end of each epoch\n",
    "        combined = list(zip(self.image_paths, self.encoded_labels))\n",
    "        np.random.shuffle(combined)\n",
    "        self.image_paths[:], self.encoded_labels[:] = zip(*combined)\n",
    "\n",
    "    def _generate_triplet_batch(self, batch_image_paths, batch_labels):\n",
    "        anchor_images = []\n",
    "        positive_images = []\n",
    "        negative_images = []\n",
    "\n",
    "        for i in range(len(batch_image_paths)):\n",
    "            anchor_path = batch_image_paths[i]\n",
    "            anchor_label = batch_labels[i]\n",
    "\n",
    "            positive_path = np.random.choice(\n",
    "                [p for p, l in zip(self.image_paths, self.encoded_labels) if l == anchor_label]\n",
    "            )\n",
    "            negative_path = np.random.choice(\n",
    "                [p for p, l in zip(self.image_paths, self.encoded_labels) if l != anchor_label]\n",
    "            )\n",
    "\n",
    "            anchor_image = load_img(anchor_path, target_size=self.image_size)\n",
    "            positive_image = load_img(positive_path, target_size=self.image_size)\n",
    "            negative_image = load_img(negative_path, target_size=self.image_size)\n",
    "\n",
    "            anchor_images.append(img_to_array(anchor_image))\n",
    "            positive_images.append(img_to_array(positive_image))\n",
    "            negative_images.append(img_to_array(negative_image))\n",
    "\n",
    "        return (\n",
    "            (\n",
    "                np.array(anchor_images),\n",
    "                np.array(positive_images),\n",
    "                np.array(negative_images)\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "# Set the directory structure\n",
    "data_dir = 'data'\n",
    "image_size = (224, 224)\n",
    "batch_size = 2  # Adjust the batch size for the small dataset\n",
    "margin = 1.0\n",
    "\n",
    "# Load and preprocess the data\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label in os.listdir(data_dir):\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        for image_name in os.listdir(label_dir):\n",
    "            image_paths.append(os.path.join(label_dir, image_name))\n",
    "            labels.append(label)\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Found {len(image_paths)} images in {len(set(labels))} classes\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Check if the splits are non-empty\n",
    "print(f\"Training on {len(train_paths)} images\")\n",
    "print(f\"Validating on {len(val_paths)} images\")\n",
    "\n",
    "# Create data generators\n",
    "num_classes = len(set(labels))\n",
    "train_generator = TripletDataGenerator(train_paths, train_labels, batch_size, image_size, num_classes)\n",
    "val_generator = TripletDataGenerator(val_paths, val_labels, batch_size, image_size, num_classes)\n",
    "\n",
    "# Check if the generators have data\n",
    "assert len(train_generator) > 0, \"Training generator is empty!\"\n",
    "assert len(val_generator) > 0, \"Validation generator is empty!\"\n",
    "\n",
    "# Create the embedding model and the Siamese network\n",
    "embedding_model = get_embedding_module(image_size)\n",
    "siamese_network = get_siamese_network(image_size, embedding_model)\n",
    "\n",
    "# Initialize the Siamese model\n",
    "loss_tracker = Mean(name=\"loss\")\n",
    "siamese_model = SiameseModel(siamese_network, margin, loss_tracker)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(optimizer=Adam())\n",
    "\n",
    "# Train the model\n",
    "siamese_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fling 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, image_size, num_classes):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.encoded_labels = self.label_encoder.fit_transform(labels)\n",
    "        self.image_data_generator = ImageDataGenerator(preprocessing_function=resnet.preprocess_input)\n",
    "        self.on_epoch_end()\n",
    "        print(f\"Initialized TripletDataGenerator with {len(self.image_paths)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.image_paths) // self.batch_size)  # Ensure at least one batch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_image_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.encoded_labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self._generate_triplet_batch(batch_image_paths, batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle the data at the end of each epoch\n",
    "        combined = list(zip(self.image_paths, self.encoded_labels))\n",
    "        np.random.shuffle(combined)\n",
    "        self.image_paths[:], self.encoded_labels[:] = zip(*combined)\n",
    "\n",
    "    def _generate_triplet_batch(self, batch_image_paths, batch_labels):\n",
    "        anchor_images = []\n",
    "        positive_images = []\n",
    "        negative_images = []\n",
    "\n",
    "        for i in range(len(batch_image_paths)):\n",
    "            anchor_path = batch_image_paths[i]\n",
    "            anchor_label = batch_labels[i]\n",
    "\n",
    "            positive_path = np.random.choice(\n",
    "                [p for p, l in zip(self.image_paths, self.encoded_labels) if l == anchor_label]\n",
    "            )\n",
    "            negative_path = np.random.choice(\n",
    "                [p for p, l in zip(self.image_paths, self.encoded_labels) if l != anchor_label]\n",
    "            )\n",
    "\n",
    "            anchor_image = load_img(anchor_path, target_size=self.image_size)\n",
    "            positive_image = load_img(positive_path, target_size=self.image_size)\n",
    "            negative_image = load_img(negative_path, target_size=self.image_size)\n",
    "\n",
    "            anchor_images.append(img_to_array(anchor_image))\n",
    "            positive_images.append(img_to_array(positive_image))\n",
    "            negative_images.append(img_to_array(negative_image))\n",
    "\n",
    "        return (\n",
    "            {\n",
    "                \"anchor\": np.array(anchor_images),\n",
    "                \"positive\": np.array(positive_images),\n",
    "                \"negative\": np.array(negative_images)\n",
    "            },\n",
    "            None,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(keras.Model):\n",
    "    def __init__(self, siameseNetwork, margin, lossTracker):\n",
    "        super().__init__()\n",
    "        self.siameseNetwork = siameseNetwork\n",
    "        self.margin = margin\n",
    "        self.lossTracker = lossTracker\n",
    "\n",
    "    def _compute_distance(self, inputs):\n",
    "        anchor, positive, negative = inputs[\"anchor\"], inputs[\"positive\"], inputs[\"negative\"]\n",
    "        # embed the images using the siamese network\n",
    "        embeddings = self.siameseNetwork((anchor, positive, negative))\n",
    "        anchorEmbedding = embeddings[0]\n",
    "        positiveEmbedding = embeddings[1]\n",
    "        negativeEmbedding = embeddings[2]\n",
    "        # calculate the anchor to positive and negative distance\n",
    "        apDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - positiveEmbedding), axis=-1\n",
    "        )\n",
    "        anDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - negativeEmbedding), axis=-1\n",
    "        )\n",
    "        # return the distances\n",
    "        return (apDistance, anDistance)\n",
    "\n",
    "    def _compute_loss(self, apDistance, anDistance):\n",
    "        loss = apDistance - anDistance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        apDistance, anDistance = self._compute_distance(inputs)\n",
    "        return (apDistance, anDistance)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute the distance between the anchor and positive,\n",
    "            # negative images\n",
    "            apDistance, anDistance = self._compute_distance(inputs)\n",
    "            # calculate the loss of the siamese network\n",
    "            loss = self._compute_loss(apDistance, anDistance)\n",
    "        # compute the gradients and optimize the model\n",
    "        gradients = tape.gradient(\n",
    "            loss,\n",
    "            self.siameseNetwork.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siameseNetwork.trainable_variables)\n",
    "        )\n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        apDistance, anDistance = self._compute_distance(inputs)\n",
    "        # calculate the loss of the siamese network\n",
    "        loss = self._compute_loss(apDistance, anDistance)\n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.lossTracker]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"siameseNetwork\": self.siameseNetwork,\n",
    "            \"margin\": self.margin,\n",
    "            \"lossTracker\": self.lossTracker\n",
    "            })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nblod6pu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/batch_step</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>batch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss</td><td>▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/batch_step</td><td>95</td></tr><tr><td>batch/learning_rate</td><td>0.001</td></tr><tr><td>batch/loss</td><td>0.0</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.0</td></tr><tr><td>epoch/val_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-microwave-4</strong> at: <a href='https://wandb.ai/devasy/FaceRec/runs/nblod6pu' target=\"_blank\">https://wandb.ai/devasy/FaceRec/runs/nblod6pu</a><br/> View project at: <a href='https://wandb.ai/devasy/FaceRec' target=\"_blank\">https://wandb.ai/devasy/FaceRec</a><br/>Synced 5 W&B file(s), 0 media file(s), 100 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240608_151613-nblod6pu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nblod6pu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\Model-Training\\wandb\\run-20240608_160612-645ek64d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devasy/FaceRec/runs/645ek64d' target=\"_blank\">gallant-plant-6</a></strong> to <a href='https://wandb.ai/devasy/FaceRec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devasy/FaceRec' target=\"_blank\">https://wandb.ai/devasy/FaceRec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devasy/FaceRec/runs/645ek64d' target=\"_blank\">https://wandb.ai/devasy/FaceRec/runs/645ek64d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images in 2 classes\n",
      "Training on 4 images\n",
      "Validating on 2 images\n",
      "Initialized TripletDataGenerator with 4 images\n",
      "Initialized TripletDataGenerator with 2 images\n",
      "Epoch 1/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0817 INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 65s 51s/step - loss: 3.0817 - val_loss: 0.0000e+00\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 43s 42s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 39s 38s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 29.2242   INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 39s 39s/step - loss: 29.2242 - val_loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 43s 43s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 39s 38s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 69.6468   INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 41s 40s/step - loss: 69.6468 - val_loss: 0.0000e+00\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 54s 54s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 44s 43s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 42s 42s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 48s 47s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 46s 45s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 48s 48s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 50s 49s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 49s 48s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 48s 47s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 37s 36s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 103.9765  INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 47s 47s/step - loss: 103.9765 - val_loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 51s 50s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0000e+00INFO:tensorflow:Assets written to: models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\models)... Done. 0.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 50s 50s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22720a37650>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the directory structure\n",
    "data_dir = 'data'\n",
    "image_size = (224, 224)\n",
    "batch_size = 2  # Adjust the batch size for the small dataset\n",
    "margin = 1.0\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"FaceRec\", config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 2,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"architecture\": \"ResNet50\",\n",
    "    \"dataset\": \"lfw\",\n",
    "    \"loss\": \"TripletLoss\",\n",
    "    \"margin\": 0.6\n",
    "})\n",
    "# Load and preprocess the data\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label in os.listdir(data_dir):\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        for image_name in os.listdir(label_dir):\n",
    "            image_paths.append(os.path.join(label_dir, image_name))\n",
    "            labels.append(label)\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Found {len(image_paths)} images in {len(set(labels))} classes\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Check if the splits are non-empty\n",
    "print(f\"Training on {len(train_paths)} images\")\n",
    "print(f\"Validating on {len(val_paths)} images\")\n",
    "\n",
    "# Create data generators\n",
    "num_classes = len(set(labels))\n",
    "train_generator = TripletDataGenerator(train_paths, train_labels, batch_size, image_size, num_classes)\n",
    "val_generator = TripletDataGenerator(val_paths, val_labels, batch_size, image_size, num_classes)\n",
    "\n",
    "# Check if the generators have data\n",
    "assert len(train_generator) > 0, \"Training generator is empty!\"\n",
    "assert len(val_generator) > 0, \"Validation generator is empty!\"\n",
    "\n",
    "# Create the embedding model and the Siamese network\n",
    "embedding_model = get_embedding_module(image_size)\n",
    "siamese_network = get_siamese_network(image_size, embedding_model)\n",
    "\n",
    "# Initialize the Siamese model\n",
    "loss_tracker = Mean(name=\"loss\")\n",
    "siamese_model = SiameseModel(siamese_network, margin, loss_tracker)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(optimizer=Adam())\n",
    "\n",
    "# Train the model\n",
    "siamese_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[WandbMetricsLogger(log_freq=5), WandbModelCheckpoint(\"models\")]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Devasy\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cdliwuhd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-forest-2</strong> at: <a href='https://wandb.ai/devasy/FaceRec/runs/cdliwuhd' target=\"_blank\">https://wandb.ai/devasy/FaceRec/runs/cdliwuhd</a><br/> View project at: <a href='https://wandb.ai/devasy/FaceRec' target=\"_blank\">https://wandb.ai/devasy/FaceRec</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240608_151325-cdliwuhd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cdliwuhd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\Model-Training\\wandb\\run-20240608_151420-9sa3dm9n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/devasy/FaceRec/runs/9sa3dm9n' target=\"_blank\">restful-glade-3</a></strong> to <a href='https://wandb.ai/devasy/FaceRec' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/devasy/FaceRec' target=\"_blank\">https://wandb.ai/devasy/FaceRec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/devasy/FaceRec/runs/9sa3dm9n' target=\"_blank\">https://wandb.ai/devasy/FaceRec/runs/9sa3dm9n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images in 2 classes\n",
      "Training on 4 images\n",
      "Validating on 2 images\n",
      "Initialized TripletDataGenerator with 4 images\n",
      "Initialized TripletDataGenerator with 2 images\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Devasy\\AppData\\Local\\Temp\\ipykernel_19076\\2185040975.py\", line 183, in train_step\n        wandb.log({\"loss\": self.lossTracker.result()})  # Log loss to W&B\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 449, in wrapper\n        return func(self, *args, **kwargs)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 400, in wrapper_fn\n        return func(self, *args, **kwargs)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 390, in wrapper\n        return func(self, *args, **kwargs)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1877, in log\n        self._log(data=data, step=step, commit=commit)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1641, in _log\n        self._partial_history_callback(data, step, commit)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1513, in _partial_history_callback\n        self._backend.interface.publish_partial_history(\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 612, in publish_partial_history\n        item.value_json = json_dumps_safer_history(v)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py\", line 839, in json_dumps_safer_history\n        return dumps(obj, cls=WandBHistoryJSONEncoder, **kwargs)\n    File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py\", line 238, in dumps\n        **kw).encode(obj)\n    File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\encoder.py\", line 200, in encode\n        chunks = self.iterencode(o, _one_shot=True)\n    File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\encoder.py\", line 258, in iterencode\n        return _iterencode(o, 0)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py\", line 800, in default\n        obj, converted = json_friendly(obj)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py\", line 594, in json_friendly\n        obj = obj.eval()\n\n    ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 251\u001b[0m\n\u001b[0;32m    248\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam())\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# Train the model with W&B callback\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mWandbMetricsLogger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWandbModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Finish the W&B run\u001b[39;00m\n\u001b[0;32m    259\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexgsjzasl.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 183\u001b[0m, in \u001b[0;36mSiameseModel.train_step\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# update the metrics and return the loss\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlossTracker\u001b[38;5;241m.\u001b[39mupdate_state(loss)\n\u001b[1;32m--> 183\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlossTracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Log loss to W&B\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlossTracker\u001b[38;5;241m.\u001b[39mresult()}\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:449\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:400\u001b[0m, in \u001b[0;36m_run_decorator._noop_on_finish.<locals>.decorator_fn.<locals>.wrapper_fn\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 400\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m     default_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m     resolved_message \u001b[38;5;241m=\u001b[39m message \u001b[38;5;129;01mor\u001b[39;00m default_message\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:390\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1877\u001b[0m, in \u001b[0;36mRun.log\u001b[1;34m(self, data, step, commit, sync)\u001b[0m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_shared \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1871\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[0;32m   1872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn shared mode, the use of `wandb.log` with the step argument is not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1873\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be ignored. Please refer to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwburls\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_define_metric\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1874\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon how to customize your x-axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1875\u001b[0m         repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1876\u001b[0m     )\n\u001b[1;32m-> 1877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1641\u001b[0m, in \u001b[0;36mRun._log\u001b[1;34m(self, data, step, commit)\u001b[0m\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[0;32m   1639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey values passed to `wandb.log` must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1641\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_history_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetpid() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pid \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attached:\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1513\u001b[0m, in \u001b[0;36mRun._partial_history_callback\u001b[1;34m(self, row, step, commit)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface:\n\u001b[0;32m   1511\u001b[0m     not_using_tensorboard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(wandb\u001b[38;5;241m.\u001b[39mpatched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1513\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_partial_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublish_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_using_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py:612\u001b[0m, in \u001b[0;36mInterfaceBase.publish_partial_history\u001b[1;34m(self, data, user_step, step, flush, publish_step, run)\u001b[0m\n\u001b[0;32m    610\u001b[0m     item \u001b[38;5;241m=\u001b[39m partial_history\u001b[38;5;241m.\u001b[39mitem\u001b[38;5;241m.\u001b[39madd()\n\u001b[0;32m    611\u001b[0m     item\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m k\n\u001b[1;32m--> 612\u001b[0m     item\u001b[38;5;241m.\u001b[39mvalue_json \u001b[38;5;241m=\u001b[39m \u001b[43mjson_dumps_safer_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m publish_step \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    615\u001b[0m     partial_history\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mnum \u001b[38;5;241m=\u001b[39m step\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py:839\u001b[0m, in \u001b[0;36mjson_dumps_safer_history\u001b[1;34m(obj, **kwargs)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjson_dumps_safer_history\u001b[39m(obj: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    838\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert obj to json, with some extra encodable types, including histograms.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWandBHistoryJSONEncoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py:800\u001b[0m, in \u001b[0;36mWandBHistoryJSONEncoder.default\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 800\u001b[0m     obj, converted \u001b[38;5;241m=\u001b[39m \u001b[43mjson_friendly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m     obj, compressed \u001b[38;5;241m=\u001b[39m maybe_compress_history(obj)\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converted:\n",
      "File \u001b[1;32mc:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py:594\u001b[0m, in \u001b[0;36mjson_friendly\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_tf_tensor_typename(typename):\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m    596\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Devasy\\AppData\\Local\\Temp\\ipykernel_19076\\2185040975.py\", line 183, in train_step\n        wandb.log({\"loss\": self.lossTracker.result()})  # Log loss to W&B\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 449, in wrapper\n        return func(self, *args, **kwargs)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 400, in wrapper_fn\n        return func(self, *args, **kwargs)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 390, in wrapper\n        return func(self, *args, **kwargs)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1877, in log\n        self._log(data=data, step=step, commit=commit)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1641, in _log\n        self._partial_history_callback(data, step, commit)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1513, in _partial_history_callback\n        self._backend.interface.publish_partial_history(\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 612, in publish_partial_history\n        item.value_json = json_dumps_safer_history(v)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py\", line 839, in json_dumps_safer_history\n        return dumps(obj, cls=WandBHistoryJSONEncoder, **kwargs)\n    File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py\", line 238, in dumps\n        **kw).encode(obj)\n    File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\encoder.py\", line 200, in encode\n        chunks = self.iterencode(o, _one_shot=True)\n    File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\encoder.py\", line 258, in iterencode\n        return _iterencode(o, 0)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py\", line 800, in default\n        obj, converted = json_friendly(obj)\n    File \"c:\\Users\\Devasy\\OneDrive\\Documents\\GitHub\\FaceRec\\venv\\Lib\\site-packages\\wandb\\util.py\", line 594, in json_friendly\n        obj = obj.eval()\n\n    ValueError: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Mean\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import resnet\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"FaceRec\", config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 2,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"architecture\": \"ResNet50\",\n",
    "    \"dataset\": \"lfw\",\n",
    "    \"loss\": \"TripletLoss\",\n",
    "    \"margin\": 0.6\n",
    "})\n",
    "config = wandb.config\n",
    "def get_embedding_module(imageSize):\n",
    "    # construct the input layer and pass the inputs through a\n",
    "    # pre-processing layer\n",
    "    inputs = keras.Input(imageSize + (3,))\n",
    "    x = resnet.preprocess_input(inputs)\n",
    "    \n",
    "    # fetch the pre-trained resnet 50 model and freeze the weights\n",
    "    baseCnn = resnet.ResNet50(weights=\"imagenet\", include_top=False)\n",
    "    baseCnn.trainable = False\n",
    "    \n",
    "    # pass the pre-processed inputs through the base cnn and get the\n",
    "    # extracted features from the inputs\n",
    "    extractedFeatures = baseCnn(x)\n",
    "    # pass the extracted features through a number of trainable layers\n",
    "    x = layers.GlobalAveragePooling2D()(extractedFeatures)\n",
    "    x = layers.Dense(units=1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(units=512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(units=128)(x)\n",
    "    # build the embedding model and return it\n",
    "    embedding = keras.Model(inputs, outputs, name=\"embedding\")\n",
    "    return embedding\n",
    "\n",
    "def get_siamese_network(imageSize, embeddingModel):\n",
    "    # build the anchor, positive and negative input layer\n",
    "    anchorInput = keras.Input(name=\"anchor\", shape=imageSize + (3,))\n",
    "    positiveInput = keras.Input(name=\"positive\", shape=imageSize + (3,))\n",
    "    negativeInput = keras.Input(name=\"negative\", shape=imageSize + (3,))\n",
    "    # embed the anchor, positive and negative images\n",
    "    anchorEmbedding = embeddingModel(anchorInput)\n",
    "    positiveEmbedding = embeddingModel(positiveInput)\n",
    "    negativeEmbedding = embeddingModel(negativeInput)\n",
    "    # build the siamese network and return it\n",
    "    siamese_network = keras.Model(\n",
    "        inputs=[anchorInput, positiveInput, negativeInput],\n",
    "        outputs=[anchorEmbedding, positiveEmbedding, negativeEmbedding]\n",
    "    )\n",
    "    return siamese_network\n",
    "\n",
    "class TripletDataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, image_size, num_classes):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.encoded_labels = self.label_encoder.fit_transform(labels)\n",
    "        self.image_data_generator = ImageDataGenerator(preprocessing_function=resnet.preprocess_input)\n",
    "        self.on_epoch_end()\n",
    "        print(f\"Initialized TripletDataGenerator with {len(self.image_paths)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.image_paths) // self.batch_size)  # Ensure at least one batch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_image_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.encoded_labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        return self._generate_triplet_batch(batch_image_paths, batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle the data at the end of each epoch\n",
    "        combined = list(zip(self.image_paths, self.encoded_labels))\n",
    "        np.random.shuffle(combined)\n",
    "        self.image_paths[:], self.encoded_labels[:] = zip(*combined)\n",
    "\n",
    "    def _generate_triplet_batch(self, batch_image_paths, batch_labels):\n",
    "        anchor_images = []\n",
    "        positive_images = []\n",
    "        negative_images = []\n",
    "\n",
    "        for i in range(len(batch_image_paths)):\n",
    "            anchor_path = batch_image_paths[i]\n",
    "            anchor_label = batch_labels[i]\n",
    "\n",
    "            positive_path = np.random.choice(\n",
    "                [p for p, l in zip(self.image_paths, self.encoded_labels) if l == anchor_label]\n",
    "            )\n",
    "            negative_path = np.random.choice(\n",
    "                [p for p, l in zip(self.image_paths, self.encoded_labels) if l != anchor_label]\n",
    "            )\n",
    "\n",
    "            anchor_image = load_img(anchor_path, target_size=self.image_size)\n",
    "            positive_image = load_img(positive_path, target_size=self.image_size)\n",
    "            negative_image = load_img(negative_path, target_size=self.image_size)\n",
    "\n",
    "            anchor_images.append(img_to_array(anchor_image))\n",
    "            positive_images.append(img_to_array(positive_image))\n",
    "            negative_images.append(img_to_array(negative_image))\n",
    "\n",
    "        return (\n",
    "            {\n",
    "                \"anchor\": np.array(anchor_images),\n",
    "                \"positive\": np.array(positive_images),\n",
    "                \"negative\": np.array(negative_images)\n",
    "            },\n",
    "            None,\n",
    "        )\n",
    "\n",
    "class SiameseModel(keras.Model):\n",
    "    def __init__(self, siameseNetwork, margin, lossTracker):\n",
    "        super().__init__()\n",
    "        self.siameseNetwork = siameseNetwork\n",
    "        self.margin = margin\n",
    "        self.lossTracker = lossTracker\n",
    "\n",
    "    def _compute_distance(self, inputs):\n",
    "        anchor, positive, negative = inputs[\"anchor\"], inputs[\"positive\"], inputs[\"negative\"]\n",
    "        # embed the images using the siamese network\n",
    "        embeddings = self.siameseNetwork((anchor, positive, negative))\n",
    "        anchorEmbedding = embeddings[0]\n",
    "        positiveEmbedding = embeddings[1]\n",
    "        negativeEmbedding = embeddings[2]\n",
    "        # calculate the anchor to positive and negative distance\n",
    "        apDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - positiveEmbedding), axis=-1\n",
    "        )\n",
    "        anDistance = tf.reduce_sum(\n",
    "            tf.square(anchorEmbedding - negativeEmbedding), axis=-1\n",
    "        )\n",
    "        # return the distances\n",
    "        return (apDistance, anDistance)\n",
    "\n",
    "    def _compute_loss(self, apDistance, anDistance):\n",
    "        loss = apDistance - anDistance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        apDistance, anDistance = self._compute_distance(inputs)\n",
    "        return (apDistance, anDistance)\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # compute the distance between the anchor and positive,\n",
    "            # negative images\n",
    "            apDistance, anDistance = self._compute_distance(inputs)\n",
    "            # calculate the loss of the siamese network\n",
    "            loss = self._compute_loss(apDistance, anDistance)\n",
    "        # compute the gradients and optimize the model\n",
    "        gradients = tape.gradient(\n",
    "            loss,\n",
    "            self.siameseNetwork.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siameseNetwork.trainable_variables)\n",
    "        )\n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        wandb.log({\"loss\": self.lossTracker.result()})  # Log loss to W&B\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        # compute the distance between the anchor and positive,\n",
    "        # negative images\n",
    "        apDistance, anDistance = self._compute_distance(inputs)\n",
    "        # calculate the loss of the siamese network\n",
    "        loss = self._compute_loss(apDistance, anDistance)\n",
    "        # update the metrics and return the loss\n",
    "        self.lossTracker.update_state(loss)\n",
    "        wandb.log({\"val_loss\": self.lossTracker.result()})  # Log validation loss to W&B\n",
    "        return {\"loss\": self.lossTracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.lossTracker]\n",
    "\n",
    "# Set the directory structure\n",
    "data_dir = 'data'\n",
    "image_size = (224, 224)\n",
    "batch_size = 2  # Adjust the batch size for the small dataset\n",
    "margin = 1.0\n",
    "\n",
    "# Load and preprocess the data\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label in os.listdir(data_dir):\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        for image_name in os.listdir(label_dir):\n",
    "            image_paths.append(os.path.join(label_dir, image_name))\n",
    "            labels.append(label)\n",
    "\n",
    "# Debugging output\n",
    "print(f\"Found {len(image_paths)} images in {len(set(labels))} classes\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Check if the splits are non-empty\n",
    "print(f\"Training on {len(train_paths)} images\")\n",
    "print(f\"Validating on {len(val_paths)} images\")\n",
    "\n",
    "# Create data generators\n",
    "num_classes = len(set(labels))\n",
    "train_generator = TripletDataGenerator(train_paths, train_labels, batch_size, image_size, num_classes)\n",
    "val_generator = TripletDataGenerator(val_paths, val_labels, batch_size, image_size, num_classes)\n",
    "\n",
    "# Check if the generators have data\n",
    "assert len(train_generator) > 0, \"Training generator is empty!\"\n",
    "assert len(val_generator) > 0, \"Validation generator is empty!\"\n",
    "\n",
    "# Create the embedding model and the Siamese network\n",
    "embedding_model = get_embedding_module(image_size)\n",
    "siamese_network = get_siamese_network(image_size, embedding_model)\n",
    "\n",
    "# Initialize the Siamese model\n",
    "loss_tracker = Mean(name=\"loss\")\n",
    "siamese_model = SiameseModel(siamese_network, margin, loss_tracker)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(optimizer=Adam())\n",
    "\n",
    "# Train the model with W&B callback\n",
    "siamese_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[WandbMetricsLogger(log_freq=5), WandbModelCheckpoint(\"models\")]\n",
    ")\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
